{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e28961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clown2/anaconda3/envs/nanogpt/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/clown2/anaconda3/envs/nanogpt/lib/python3.12/site-packages/torch/random.py:187: UserWarning: CUDA reports that you have 2 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of CUDAs. If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using. For example, if you are using CPU only, set device.upper()_VISIBLE_DEVICES= or devices=[]; if you are using device 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clown2/anaconda3/envs/nanogpt/lib/python3.12/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/clown2/Desktop/Work/Research/nanogpt/videos/exp folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0204, -0.0013,  0.1917, -0.0016,  0.9995, -0.0063,  0.0302]]), 'goal_pos': tensor([[-0.0281,  0.0587,  0.2543]]), 'obj_pose': tensor([[-0.0208,  0.0844,  0.0200,  0.9966,  0.0000,  0.0000,  0.0823]]), 'tcp_to_obj_pos': tensor([[-0.0413,  0.0857, -0.1717]]), 'obj_to_goal_pos': tensor([[-0.0073, -0.0257,  0.2343]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[-0.0026, -0.0529,  0.1654,  0.0100,  0.9998, -0.0171, -0.0039]]), 'goal_pos': tensor([[ 0.0490, -0.0444,  0.1793]]), 'obj_pose': tensor([[0.0919, 0.0741, 0.0200, 0.4322, 0.0000, 0.0000, 0.9018]]), 'tcp_to_obj_pos': tensor([[ 0.0945,  0.1269, -0.1454]]), 'obj_to_goal_pos': tensor([[-0.0429, -0.1185,  0.1593]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[-0.0055,  0.0016,  0.1520, -0.0114,  0.9999, -0.0055, -0.0090]]), 'goal_pos': tensor([[ 0.0482, -0.0297,  0.0323]]), 'obj_pose': tensor([[-0.0877, -0.0407,  0.0200,  0.4076, -0.0000, -0.0000, -0.9132]]), 'tcp_to_obj_pos': tensor([[-0.0822, -0.0423, -0.1320]]), 'obj_to_goal_pos': tensor([[0.1359, 0.0111, 0.0123]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[-0.0025, -0.0043,  0.1724, -0.0053,  0.9999, -0.0050, -0.0120]]), 'goal_pos': tensor([[0.0795, 0.0886, 0.2227]]), 'obj_pose': tensor([[-0.0254, -0.0051,  0.0200,  0.2530,  0.0000,  0.0000,  0.9675]]), 'tcp_to_obj_pos': tensor([[-0.0229, -0.0007, -0.1524]]), 'obj_to_goal_pos': tensor([[0.1049, 0.0936, 0.2027]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[-0.0183, -0.0309,  0.1477,  0.0099,  0.9993, -0.0281, -0.0212]]), 'goal_pos': tensor([[ 0.0709, -0.0674,  0.3004]]), 'obj_pose': tensor([[-0.0360, -0.0340,  0.0200,  0.9934,  0.0000,  0.0000,  0.1146]]), 'tcp_to_obj_pos': tensor([[-0.0177, -0.0031, -0.1277]]), 'obj_to_goal_pos': tensor([[ 0.1070, -0.0334,  0.2804]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 1.1940e-02,  2.0352e-03,  1.8647e-01,  4.7127e-04,  9.9977e-01,\n",
      "         -1.2503e-02,  1.7325e-02]]), 'goal_pos': tensor([[-0.0858, -0.0307,  0.3018]]), 'obj_pose': tensor([[ 0.0562, -0.0362,  0.0200,  0.6752, -0.0000, -0.0000, -0.7377]]), 'tcp_to_obj_pos': tensor([[ 0.0442, -0.0383, -0.1665]]), 'obj_to_goal_pos': tensor([[-0.1419,  0.0055,  0.2818]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0155, -0.0188,  0.1768,  0.0076,  0.9998, -0.0023,  0.0191]]), 'goal_pos': tensor([[-0.0904,  0.0398,  0.2118]]), 'obj_pose': tensor([[ 0.0131,  0.0042,  0.0200,  0.4793, -0.0000, -0.0000, -0.8776]]), 'tcp_to_obj_pos': tensor([[-0.0024,  0.0229, -0.1568]]), 'obj_to_goal_pos': tensor([[-0.1035,  0.0357,  0.1918]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0122, -0.0162,  0.1875,  0.0109,  0.9997,  0.0059,  0.0210]]), 'goal_pos': tensor([[0.0498, 0.0654, 0.2559]]), 'obj_pose': tensor([[-0.0192, -0.0984,  0.0200,  0.9994,  0.0000,  0.0000, -0.0352]]), 'tcp_to_obj_pos': tensor([[-0.0314, -0.0822, -0.1675]]), 'obj_to_goal_pos': tensor([[0.0689, 0.1638, 0.2359]])}\n",
      "{'is_grasped': tensor([False]), 'tcp_pose': tensor([[ 0.0038,  0.0143,  0.1779, -0.0037,  0.9999,  0.0056,  0.0118]]), 'goal_pos': tensor([[-0.0639, -0.0630,  0.0602]]), 'obj_pose': tensor([[-0.0293, -0.0821,  0.0200,  0.1251,  0.0000,  0.0000,  0.9921]]), 'tcp_to_obj_pos': tensor([[-0.0331, -0.0964, -0.1579]]), 'obj_to_goal_pos': tensor([[-0.0346,  0.0191,  0.0402]])}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import mani_skill.envs\n",
    "from mani_skill.utils import gym_utils\n",
    "from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper\n",
    "from mani_skill.utils.wrappers.record import RecordEpisode\n",
    "from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv\n",
    "\n",
    "\n",
    "env_id = \"PickCube-v1\"\n",
    "seed = 69420\n",
    "capture_video = True\n",
    "run_name = \"exp\"\n",
    "num_envs = 8\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id, seed, i, capture_video, run_name) for i in range(num_envs)]\n",
    ")\n",
    "\n",
    "obs, _ = envs.reset(seed=seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b509e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (1, 42), float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
